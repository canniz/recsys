{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import random\n",
    "from scipy.sparse import linalg\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from FW_Similarity.CFW_D_Similarity_Linalg import CFW_D_Similarity_Linalg\n",
    "from Base.Evaluation.Evaluator import SequentialEvaluator\n",
    "\n",
    "TEST_SET_THRESHOLD = 10\n",
    "TEST_SET_HOLDOUT = 0.2\n",
    "BEST_ALFA = 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('../input/tracks.csv')\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "target = pd.read_csv('../input/target_playlists.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining methods to create csr matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_durations(data):\n",
    "    data.loc[tracks['duration_sec'].isin(range(60)),'duration_sec'] = 1\n",
    "    data.loc[tracks['duration_sec'].isin(range(60,120)), 'duration_sec'] = 2\n",
    "    data.loc[tracks['duration_sec'].isin(range(120,180)), 'duration_sec'] = 3\n",
    "    data.loc[tracks['duration_sec'].isin(range(180,240)), 'duration_sec'] = 4\n",
    "    data.loc[tracks['duration_sec'].isin(range(240,300)), 'duration_sec'] = 5\n",
    "    data.loc[tracks['duration_sec'].isin(range(300,200000)), 'duration_sec'] = 6\n",
    "\n",
    "def build_urm_csr(data):\n",
    "    fill_data = np.ones(data.shape[0])\n",
    "    #posso usare gli id direttamente solo perchè come già detto sono consistenti\n",
    "    row = data['playlist_id'].values\n",
    "    col = data['track_id'].values\n",
    "    n_pl = 20635\n",
    "    n_tr = np.amax(data['track_id']) + 1\n",
    "    \n",
    "    return sparse.csr_matrix((fill_data, (row, col)), dtype=float, shape=(n_pl, n_tr))\n",
    "\n",
    "def build_icm_csr(data):\n",
    "    \n",
    "    classify_durations(data)\n",
    "    \n",
    "    albums_id = data['album_id']\n",
    "    artists_id = data['artist_id']\n",
    "    tracks = data['track_id']\n",
    "    \n",
    "    albums_max = np.amax(albums_id)\n",
    "    artists_max = np.amax(artists_id)\n",
    "    number_of_songs = data.shape[0]\n",
    "    \n",
    "    icm_csr_matrix = sparse.csr_matrix((number_of_songs, albums_max + artists_max + 2), dtype=np.float32)\n",
    "   \n",
    "    icm_csr_matrix[tracks, albums_id] = 1\n",
    "    icm_csr_matrix[tracks, albums_max + artists_id] = 1\n",
    "\n",
    "    return icm_csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITTING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Raggruppiamo per playlist_id, le celle conterranno il count() del gruppo, quindi il numero di canzoni per playlist\n",
    "grouped = train.groupby('playlist_id')['track_id'].nunique()\n",
    "\n",
    "#Prendiamo le playlist che superano il numero di elementi del TEST_SET_THRESHOLD\n",
    "clipped = grouped.index[grouped>TEST_SET_THRESHOLD].tolist()\n",
    "\n",
    "#Adesso prendiamo a caso degli indici di playlist in percentuale di TEST_SET_HOLDOUT\n",
    "#ATTENZIONE, la percentuale viene calcolata sulla lunghezza di clipped, che avrà un numero di elementi inferiore a train\n",
    "#Questo significa che il 20% di clipped sarà circa il 14% del train, la percentuale è da aggiustare tenendo conto di sto fatto\n",
    "test_set_indices = [ clipped[i] for i in sorted(random.sample(range(len(clipped)), int(TEST_SET_HOLDOUT*len(clipped)))) ]\n",
    "\n",
    "#Andiamo a estrarre dal train TUTTE le canzoni delle playlist estratte a sorte nella riga prima\n",
    "test_groups = train.loc[train['playlist_id'].isin(test_set_indices)]\n",
    "\n",
    "#Andiamo a creare un dataframe vuoto, a cui appenderemo tutte le canzoni da ficcare nel test_set con una .append()\n",
    "test_set = pd.DataFrame(columns=[\"playlist_id\",\"track_id\"])\n",
    "\n",
    "#Per ogni gruppo prendiamo le ultime 10 canzoni e le appendiamo al test_set\n",
    "for name, group in test_groups.groupby('playlist_id'):\n",
    "    test_set = test_set.append(group.tail(10))\n",
    "\n",
    "#Togliamo le canzoni del test set al train, salvandolo in una nuova variabile \n",
    "#Questo è solo un trick per fare la differenza insiemistica\n",
    "training_set = pd.concat([train, test_set, test_set]).drop_duplicates(keep=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "row index exceeds matrix dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-bc819b9daaa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Ora passiamo training_set e test_set a csr_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_set_csr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_urm_csr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0micm_csr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_icm_csr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtracks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0murm_csr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_urm_csr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_set_playlists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set_csr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-57f57c423ad7>\u001b[0m in \u001b[0;36mbuild_urm_csr\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mn_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'track_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_pl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbuild_icm_csr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     49\u001b[0m                     \u001b[1;31m# (data, ij) format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                     \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36m_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnz\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'row index exceeds matrix dimensions'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'column index exceeds matrix dimensions'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: row index exceeds matrix dimensions"
     ]
    }
   ],
   "source": [
    "#Ora passiamo training_set e test_set a csr_matrix\n",
    "test_set_csr = build_urm_csr(test_set)\n",
    "icm_csr = build_icm_csr(tracks)\n",
    "urm_csr = build_urm_csr(training_set)\n",
    "test_set_playlists = np.unique(test_set_csr.nonzero()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50442, 20633)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_csr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "    \n",
    "    return precision_score\n",
    "\n",
    "def recall(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall_score\n",
    "\n",
    "def MAP(recommended_items, relevant_items):\n",
    "       \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    \n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommender_object, target_playlists, at=10, alfa = 0.9):\n",
    "    \n",
    "    \n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for user_id in target_playlists:\n",
    "    \n",
    "        target_items = URM_test.getrow(user_id).indices\n",
    "        \n",
    "        recommended_items = recommender_object.recommend(user_id, at=at, alfa = alfa)\n",
    "        num_eval+=1\n",
    "        \n",
    "        cumulative_precision += precision(recommended_items, target_items)\n",
    "        cumulative_recall += recall(recommended_items, target_items)\n",
    "        cumulative_MAP += MAP(recommended_items, target_items)\n",
    "        \n",
    "        recommendation_string = \" \".join(str(i) for i in recommended_items)\n",
    "        temp = [user_id,recommendation_string]\n",
    "        result.append(temp)\n",
    "\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "    \n",
    "    rec = pd.DataFrame(result)\n",
    "    rec.to_csv(\"sample_submission.csv\", index = False, header = [\"playlist_id\", \"track_ids\"])\n",
    "    print(\"Recommender performance is: Precision = {:.6f}, Recall = {:.6f}, MAP = {:.6f}\".format(cumulative_precision, cumulative_recall, cumulative_MAP))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleRecommender(object):\n",
    "    def get_URM_train(self):\n",
    "        return self.URM_csr\n",
    "    def fit(self, URM_csr, ICM_csr, min_common_features = 5):\n",
    "        self.min_common_features = min_common_features\n",
    "        transformer = TfidfTransformer()\n",
    "        transformer.fit(URM_csr)\n",
    "        tf_idf_csr = transformer.transform(URM_csr)\n",
    "\n",
    "        IRM = sparse.csr_matrix(tf_idf_csr.transpose())\n",
    "        \n",
    "        csr_similarities = sparse.csr_matrix(cosine_similarity(IRM, dense_output=False))\n",
    "        \n",
    "\n",
    "        transformer.fit(ICM_csr)\n",
    "        tf_idf_icm = transformer.transform(ICM_csr)\n",
    "        icm_similarities = sparse.csr_matrix(cosine_similarity(tf_idf_icm, dense_output=False))\n",
    "        \n",
    "        print(\"COMPUTING ENSEMBLED CONTENT SIMILARITIES\")\n",
    "        #self.item_similarities = alfa*csr_similarities + (1-alfa)*icm_similarities  \n",
    "        \n",
    "        # Get common structure\n",
    "        W_sparse_CF_structure = icm_similarities.copy()\n",
    "        W_sparse_CF_structure.data = np.ones_like(W_sparse_CF_structure.data)\n",
    "\n",
    "        W_sparse_CBF_structure = csr_similarities.copy()\n",
    "        W_sparse_CBF_structure.data = np.ones_like(W_sparse_CBF_structure.data)\n",
    "\n",
    "        W_sparse_common = W_sparse_CF_structure.multiply(W_sparse_CBF_structure)\n",
    "\n",
    "        # Get values of both in common structure of CF\n",
    "        W_sparse_delta = icm_similarities.multiply(W_sparse_common)\n",
    "        W_sparse_delta -= csr_similarities.multiply(W_sparse_common)\n",
    "        \n",
    "        W_sparse_delta_sorted = np.sort(W_sparse_delta.data.copy())\n",
    "        \n",
    "        print(\"CREATING CFW...\")\n",
    "        self.CFW_weithing = CFW_D_Similarity_Linalg(URM_csr, ICM_csr, csr_similarities)\n",
    "        print(\"FITTING CFW...\")\n",
    "        self.CFW_weithing.fit()\n",
    "        self.URM_csr = URM_csr\n",
    "        \n",
    "    \n",
    "    def recommend(self, user_id, at=10, remove_seen_flag=True, alfa = 0.9):\n",
    "        \n",
    "        user = self.URM_csr.getrow(user_id)\n",
    "        itemPopularity = user.dot(self.icm_similarities) + user.dot(self.csr_similarities)\n",
    "        popularItems = np.argsort(np.array(itemPopularity.todense())[0])\n",
    "        popularItems = np.flip(popularItems, axis = 0)\n",
    "\n",
    "        if remove_seen_flag:\n",
    "            unseen_items_mask = np.in1d(popularItems, self.URM_csr[user_id].indices,\n",
    "                                        assume_unique=True, invert = True)\n",
    "\n",
    "            unseen_items = popularItems[unseen_items_mask]\n",
    "            \n",
    "            recommended_items = unseen_items[0:at]\n",
    "\n",
    "        else:\n",
    "            recommended_items = popularItems[0:at]\n",
    "            \n",
    "        #recommended_items = \" \".join(str(i) for i in recommended_items)\n",
    "        return recommended_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FITTING...\n",
      "COMPUTING ENSEMBLED CONTENT SIMILARITIES\n",
      "CREATING CFW...\n",
      "FITTING CFW...\n",
      "CFW_D_Similarity_Linalg: Generating train data\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 20600 ( 100 % ), 2067.99 column/sec, elapsed time 0.17 min\n",
      "CFW_D_Similarity_Linalg: Collaborative S density: 3.16E-02, nonzero cells 13437460\n",
      "CFW_D_Similarity_Linalg: Content S density: 5.43E-04, nonzero cells 231018\n",
      "CFW_D_Similarity_Linalg: Content S structure has 166784 out of 231018 ( 72.20%) nonzero collaborative cells\n",
      "CFW_D_Similarity_Linalg: Nonzero collaborative cell sum is: 2.45E+04, average is: 1.47E-01, average over all collaborative data is 2.24E-02\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 20600 ( 100 % ), 1512.19 column/sec, elapsed time 0.23 min\n"
     ]
    }
   ],
   "source": [
    "ensemble = EnsembleRecommender()\n",
    "test_cf = [1,2,3,4,5,6,7,8,9,10]\n",
    "print(\"FITTING...\")\n",
    "ensemble.fit(urm_csr,icm_csr)\n",
    "\n",
    "#evaluate_algorithm(test_set_csr, ensemble, test_set_playlists, alfa = test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING...\n",
      "SequentialEvaluator: Processed 7757 ( 100.00% ) in 3.72 seconds. Users per second: 2085\n",
      "{1: {'ROC_AUC': 0.11602423617377852, 'PRECISION': 0.11602423617377852, 'RECALL': 0.011602423617377745, 'RECALL_TEST_LEN': 0.11602423617377852, 'MAP': 0.11602423617377852, 'MRR': 0.11602423617377852, 'NDCG': 0.025535978360536928, 'F1': 0.021095315667959556, 'HIT_RATE': 0.11602423617377852, 'ARHR': 0.11602423617377852, 'NOVELTY': 0.000695790676895218, 'DIVERSITY_MEAN_INTER_LIST': 0.998419135708199, 'DIVERSITY_HERFINDAHL': 0.9982904236886414, 'COVERAGE_ITEM': 0.12077739543449813, 'COVERAGE_USER': 0.153792775288473, 'DIVERSITY_GINI': 0.45549149065106936, 'SHANNON_ENTROPY': 10.31636456473925}}\n",
      "EVALUATING...\n",
      "SequentialEvaluator: Processed 7757 ( 100.00% ) in 3.95 seconds. Users per second: 1964\n",
      "{2: {'ROC_AUC': 0.11602423617377852, 'PRECISION': 0.11028748227407503, 'RECALL': 0.022057496454814522, 'RECALL_TEST_LEN': 0.11028748227407503, 'MAP': 0.09507541575351296, 'MRR': 0.14644836921490267, 'NDCG': 0.040054147973016964, 'F1': 0.03676249409135767, 'HIT_RATE': 0.22057496454815007, 'ARHR': 0.16829960036096428, 'NOVELTY': 0.001395589864407458, 'DIVERSITY_MEAN_INTER_LIST': 0.9981616198794441, 'DIVERSITY_HERFINDAHL': 0.9990164705288751, 'COVERAGE_ITEM': 0.205156787670237, 'COVERAGE_USER': 0.153792775288473, 'DIVERSITY_GINI': 0.42984628935274094, 'SHANNON_ENTROPY': 11.036782379922261}}\n",
      "EVALUATING...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20634 is out of bounds for axis 0 with size 20633",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-44e81343ed2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mevaluator_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequentialEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set_csr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"EVALUATING...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mresults_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluator_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluateRecommender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCFW_weithing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\recsys\\Notebooks\\Base\\Evaluation\\Evaluator.py\u001b[0m in \u001b[0;36mevaluateRecommender\u001b[1;34m(self, recommender_object)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m         \u001b[0mresults_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_users_evaluated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_evaluation_on_selected_users\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecommender_object\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musersToEvaluate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\recsys\\Notebooks\\Base\\Evaluation\\Evaluator.py\u001b[0m in \u001b[0;36m_run_evaluation_on_selected_users\u001b[1;34m(self, recommender_object, usersToEvaluate, block_size)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                     \u001b[0mresults_current_cutoff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEvaluatorMetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOVELTY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_recommendations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecommended_items_current_cutoff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m                     \u001b[0mresults_current_cutoff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEvaluatorMetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDIVERSITY_GINI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_recommendations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecommended_items_current_cutoff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m                     \u001b[0mresults_current_cutoff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEvaluatorMetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSHANNON_ENTROPY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_recommendations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecommended_items_current_cutoff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                     \u001b[0mresults_current_cutoff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEvaluatorMetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOVERAGE_ITEM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_recommendations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecommended_items_current_cutoff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\recsys\\Notebooks\\Base\\Evaluation\\metrics.py\u001b[0m in \u001b[0;36madd_recommendations\u001b[1;34m(self, recommended_items_ids)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_recommendations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecommended_items_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecommended_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrecommended_items_ids\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_metric_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 20634 is out of bounds for axis 0 with size 20633"
     ]
    }
   ],
   "source": [
    "for test in test_cf:\n",
    "    evaluator_test = SequentialEvaluator(test_set_csr, cutoff_list=[test])\n",
    "    print(\"EVALUATING...\")\n",
    "    results_dict, _ = evaluator_test.evaluateRecommender(ensemble.CFW_weithing)\n",
    "    print(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING SINGLE ITERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
