{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "TEST_SET_THRESHOLD = 10\n",
    "TEST_SET_HOLDOUT = 0.2\n",
    "K_THRESHOLD_BEST = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracks = pd.read_csv('../input/tracks.csv')\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "target = pd.read_csv('../input/target_playlists.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining methods to create csr matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_durations(data):\n",
    "    data.loc[tracks['duration_sec'].isin(range(60)),'duration_sec'] = 1\n",
    "    data.loc[tracks['duration_sec'].isin(range(60,120)), 'duration_sec'] = 2\n",
    "    data.loc[tracks['duration_sec'].isin(range(120,180)), 'duration_sec'] = 3\n",
    "    data.loc[tracks['duration_sec'].isin(range(180,240)), 'duration_sec'] = 4\n",
    "    data.loc[tracks['duration_sec'].isin(range(240,300)), 'duration_sec'] = 5\n",
    "    data.loc[tracks['duration_sec'].isin(range(300,200000)), 'duration_sec'] = 6\n",
    "\n",
    "def build_urm_csr(data):\n",
    "    fill_data = np.ones(data.shape[0])\n",
    "    #posso usare gli id direttamente solo perchè come già detto sono consistenti\n",
    "    row = data['playlist_id'].values\n",
    "    col = data['track_id'].values\n",
    "    n_pl = np.amax(data['playlist_id']) + 1\n",
    "    n_tr = np.amax(data['track_id']) + 1\n",
    "    \n",
    "    return sparse.csr_matrix((fill_data, (row, col)), dtype=float, shape=(n_pl, n_tr))\n",
    "\n",
    "def build_icm_csr(data):\n",
    "    \n",
    "    classify_durations(data)\n",
    "    \n",
    "    albums_id = data['album_id']\n",
    "    artists_id = data['artist_id']\n",
    "    durations = data['duration_sec']\n",
    "    tracks = data['track_id']\n",
    "    \n",
    "    albums_max = np.amax(albums_id)\n",
    "    artists_max = np.amax(artists_id)\n",
    "    durations_max = np.amax(durations)\n",
    "    number_of_songs = data.shape[0]\n",
    "    \n",
    "    icm_csr_matrix = sparse.csr_matrix((number_of_songs, albums_max + artists_max + durations_max + 3), dtype=np.uint32)\n",
    "    \n",
    "    icm_csr_matrix[tracks,albums_id] = 1\n",
    "    icm_csr_matrix[tracks, albums_max + artists_id] = 1\n",
    "    icm_csr_matrix[tracks, albums_max + artists_max + durations] = 1\n",
    "\n",
    "    return icm_csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITTING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raggruppiamo per playlist_id, le celle conterranno il count() del gruppo, quindi il numero di canzoni per playlist\n",
    "grouped = train.groupby('playlist_id')['track_id'].nunique()\n",
    "\n",
    "#Prendiamo le playlist che superano il numero di elementi del TEST_SET_THRESHOLD\n",
    "clipped = grouped.index[grouped>TEST_SET_THRESHOLD].tolist()\n",
    "\n",
    "#Adesso prendiamo a caso degli indici di playlist in percentuale di TEST_SET_HOLDOUT\n",
    "#ATTENZIONE, la percentuale viene calcolata sulla lunghezza di clipped, che avrà un numero di elementi inferiore a train\n",
    "#Questo significa che il 20% di clipped sarà circa il 14% del train, la percentuale è da aggiustare tenendo conto di sto fatto\n",
    "test_set_indices = [ clipped[i] for i in sorted(random.sample(range(len(clipped)), int(TEST_SET_HOLDOUT*len(clipped)))) ]\n",
    "\n",
    "#Andiamo a estrarre dal train TUTTE le canzoni delle playlist estratte a sorte nella riga prima\n",
    "test_groups = train.loc[train['playlist_id'].isin(test_set_indices)]\n",
    "\n",
    "#Andiamo a creare un dataframe vuoto, a cui appenderemo tutte le canzoni da ficcare nel test_set con una .append()\n",
    "test_set = pd.DataFrame(columns=[\"playlist_id\",\"track_id\"])\n",
    "\n",
    "#Per ogni gruppo prendiamo le ultime 10 canzoni e le appendiamo al test_set\n",
    "for name, group in test_groups.groupby('playlist_id'):\n",
    "    test_set = test_set.append(group.tail(10))\n",
    "    \n",
    "#Togliamo le canzoni del test set al train, salvandolo in una nuova variabile \n",
    "#Questo è solo un trick per fare la differenza insiemistica\n",
    "training_set = pd.concat([train, test_set, test_set]).drop_duplicates(keep=False)\n",
    "\n",
    "#Ora passiamo training_set e test_set a csr_matrix\n",
    "urm_csr = build_urm_csr(training_set)\n",
    "test_set_csr = build_urm_csr(test_set)\n",
    "#icm_csr = build_icm_csr(tracks)\n",
    "\n",
    "test_set_playlists = test_set['playlist_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "    \n",
    "    return precision_score\n",
    "\n",
    "def recall(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall_score\n",
    "\n",
    "def MAP(recommended_items, relevant_items):\n",
    "       \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    \n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommender_object, target_playlists, at=10):\n",
    "    \n",
    "    \n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for user_id in target_playlists:\n",
    "    \n",
    "        target_items = URM_test.getrow(user_id).indices\n",
    "        \n",
    "        recommended_items = recommender_object.recommend(user_id, at=at)\n",
    "        num_eval+=1\n",
    "        \n",
    "        cumulative_precision += precision(recommended_items, target_items)\n",
    "        cumulative_recall += recall(recommended_items, target_items)\n",
    "        cumulative_MAP += MAP(recommended_items, target_items)\n",
    "        \n",
    "        recommendation_string = \" \".join(str(i) for i in recommended_items)\n",
    "        temp = [user_id,recommendation_string]\n",
    "        result.append(temp)\n",
    "\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "    \n",
    "    rec = pd.DataFrame(result)\n",
    "    rec.to_csv(\"sample_submission.csv\", index = False, header = [\"playlist_id\", \"track_ids\"])\n",
    "    \n",
    "    print(\"Recommender performance is: Precision = {:.6f}, Recall = {:.6f}, MAP = {:.6f}\".format(\n",
    "        cumulative_precision, cumulative_recall, cumulative_MAP)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleRecommender(object):\n",
    "    \n",
    "    def fit(self, URM_csr, k):\n",
    "        self.k = k\n",
    "        transformer = TfidfTransformer()\n",
    "        transformer.fit(URM_csr)\n",
    "        tf_idf_matrix = transformer.transform(URM_csr)\n",
    "        \n",
    "        self.user_similarities = sparse.csr_matrix(cosine_similarity(tf_idf_matrix, dense_output=False))\n",
    "        \n",
    "        IRM = sparse.csr_matrix(tf_idf_matrix.transpose())\n",
    "        self.item_similarities = sparse.csr_matrix(cosine_similarity(IRM, dense_output=False))\n",
    "        \n",
    "        self.URM_csr = URM_csr\n",
    "        \n",
    "    \n",
    "    def recommend(self, user_id, at=10, remove_seen=True):\n",
    "        user = self.URM_csr.getrow(user_id)\n",
    "        if(user.size > self.k):\n",
    "            itemPopularity = user.dot(self.item_similarities)\n",
    "            popularItems = np.argsort(np.array(itemPopularity.todense())[0])\n",
    "            popularItems = np.flip(popularItems, axis = 0)\n",
    "        else:\n",
    "            sim = self.user_similarities.getrow(user_id)\n",
    "            itemPopularity = sim.dot(self.URM_csr)\n",
    "            popularItems = np.argsort(np.array(itemPopularity.todense())[0])\n",
    "            popularItems = np.flip(popularItems, axis = 0)\n",
    "        \n",
    "        if remove_seen:\n",
    "            unseen_items_mask = np.in1d(popularItems, self.URM_csr[user_id].indices,\n",
    "                                        assume_unique=True, invert = True)\n",
    "\n",
    "            unseen_items = popularItems[unseen_items_mask]\n",
    "            \n",
    "            recommended_items = unseen_items[0:at]\n",
    "\n",
    "        else:\n",
    "            recommended_items = popularItems[0:at]\n",
    "            \n",
    "        #recommended_items = \" \".join(str(i) for i in recommended_items)\n",
    "        return recommended_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With threshold k = 0\n",
      "Recommender performance is: Precision = 0.164522, Recall = 0.164522, MAP = 0.095229\n",
      "With threshold k = 1\n",
      "Recommender performance is: Precision = 0.165244, Recall = 0.165244, MAP = 0.095657\n",
      "With threshold k = 2\n",
      "Recommender performance is: Precision = 0.165412, Recall = 0.165412, MAP = 0.095918\n",
      "With threshold k = 3\n",
      "Recommender performance is: Precision = 0.164896, Recall = 0.164896, MAP = 0.095384\n",
      "With threshold k = 4\n",
      "Recommender performance is: Precision = 0.164161, Recall = 0.164161, MAP = 0.095030\n",
      "With threshold k = 5\n",
      "Recommender performance is: Precision = 0.163311, Recall = 0.163311, MAP = 0.094322\n",
      "With threshold k = 6\n",
      "Recommender performance is: Precision = 0.162318, Recall = 0.162318, MAP = 0.093636\n",
      "With threshold k = 7\n",
      "Recommender performance is: Precision = 0.161467, Recall = 0.161467, MAP = 0.093043\n"
     ]
    }
   ],
   "source": [
    "ensemble = EnsembleRecommender()\n",
    "ensemble.fit(urm_csr, K_THRESHOLD_BEST)\n",
    "evaluate_algorithm(test_set_csr, ensemble, test_set_playlists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING SINGLE ITERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
