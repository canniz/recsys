{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "TEST_SET_THRESHOLD = 20\n",
    "TEST_SET_HOLDOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('../input/tracks.csv')\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "target = pd.read_csv('../input/target_playlists.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITTING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_urm_csr(data):\n",
    "    fill_data = np.ones(data.shape[0])\n",
    "    #posso usare gli id direttamente solo perchè come già detto sono consistenti\n",
    "    row = data['playlist_id'].values\n",
    "    col = data['track_id'].values\n",
    "    n_pl = np.amax(data['playlist_id']) + 1\n",
    "    n_tr = np.amax(data['track_id']) + 1\n",
    "    \n",
    "    return sparse.csr_matrix((fill_data, (row, col)), dtype=np.int32, shape=(n_pl, n_tr))\n",
    "\n",
    "def build_icm_csr(data):\n",
    "    albums_id = data['album_id']\n",
    "    artists_id = data['artist_id']\n",
    "    durations = data['duration_sec']\n",
    "    tracks = data['track_id']\n",
    "    \n",
    "    albums_max = np.amax(albums_id)\n",
    "    artists_max = np.amax(artists_id)\n",
    "    durations_max = np.amax(durations)\n",
    "    number_of_songs = data.shape[0]\n",
    "    \n",
    "    icm_csr_matrix = sparse.csr_matrix((number_of_songs, albums_max + artists_max + durations_max + 3), dtype=np.uint32)\n",
    "    \n",
    "    icm_csr_matrix[tracks,albums_id] = 1\n",
    "    icm_csr_matrix[tracks, albums_max + artists_id] = 1\n",
    "    icm_csr_matrix[tracks, albums_max + artists_max + durations] = 1\n",
    "    \n",
    "    return icm_csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Raggruppiamo per playlist_id, le celle conterranno il count() del gruppo, quindi il numero di canzoni per playlist\n",
    "grouped = train.groupby('playlist_id')['track_id'].nunique()\n",
    "\n",
    "#Prendiamo le playlist che superano il numero di elementi del TEST_SET_THRESHOLD\n",
    "clipped = grouped.index[grouped>TEST_SET_THRESHOLD].tolist()\n",
    "\n",
    "#Adesso prendiamo a caso degli indici di playlist in percentuale di TEST_SET_HOLDOUT\n",
    "#ATTENZIONE, la percentuale viene calcolata sulla lunghezza di clipped, che avrà un numero di elementi inferiore a train\n",
    "#Questo significa che il 20% di clipped sarà circa il 14% del train, la percentuale è da aggiustare tenendo conto di sto fatto\n",
    "test_set_indices = [ clipped[i] for i in sorted(random.sample(range(len(clipped)), int(TEST_SET_HOLDOUT*len(clipped)))) ]\n",
    "\n",
    "#Andiamo a estrarre dal train TUTTE le canzoni delle playlist estratte a sorte nella riga prima\n",
    "test_groups = train.loc[train['playlist_id'].isin(test_set_indices)]\n",
    "\n",
    "#Andiamo a creare un dataframe vuoto, a cui appenderemo tutte le canzoni da ficcare nel test_set con una .append()\n",
    "test_set = pd.DataFrame(columns=[\"playlist_id\",\"track_id\"])\n",
    "\n",
    "#Per ogni gruppo prendiamo le ultime 10 canzoni e le appendiamo al test_set\n",
    "for name, group in test_groups.groupby('playlist_id'):\n",
    "    test_set = test_set.append(group.tail(10))\n",
    "    \n",
    "#Togliamo le canzoni del test set al train, salvandolo in una nuova variabile \n",
    "#Questo è solo un trick per fare la differenza insiemistica\n",
    "training_set = pd.concat([train, test_set, test_set]).drop_duplicates(keep=False)\n",
    "\n",
    "#Ora passiamo training_set e test_set a csr_matrix\n",
    "training_set_csr = build_urm_csr(training_set)\n",
    "test_set_csr = build_urm_csr(test_set)\n",
    "test_set_playlists = test_set['playlist_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_durations(data):\n",
    "    data.loc[tracks['duration_sec'].isin(range(60)),'duration_sec'] = 1\n",
    "    data.loc[tracks['duration_sec'].isin(range(60,120)), 'duration_sec'] = 2\n",
    "    data.loc[tracks['duration_sec'].isin(range(120,180)), 'duration_sec'] = 3\n",
    "    data.loc[tracks['duration_sec'].isin(range(180,240)), 'duration_sec'] = 4\n",
    "    data.loc[tracks['duration_sec'].isin(range(240,300)), 'duration_sec'] = 5\n",
    "    data.loc[tracks['duration_sec'].isin(range(300,200000)), 'duration_sec'] = 6\n",
    "\n",
    "    data['duration_sec'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopPopRecommender(object):\n",
    "\n",
    "    def fit(self, train):\n",
    "        \n",
    "        self.train = train\n",
    "\n",
    "        itemPopularity = (train>0).sum(axis=0)\n",
    "        itemPopularity = np.array(itemPopularity).squeeze()\n",
    "\n",
    "        # We are not interested in sorting the popularity value,\n",
    "        # but to order the items according to it\n",
    "        self.popularItems = np.argsort(itemPopularity)\n",
    "        self.popularItems = np.flip(self.popularItems, axis = 0)\n",
    "    \n",
    "    \n",
    "    def recommend(self, user_id, at=10, remove_seen=True):\n",
    "\n",
    "        if remove_seen:\n",
    "            unseen_items_mask = np.in1d(self.popularItems, self.train[user_id].indices,\n",
    "                                        assume_unique=True, invert = True)\n",
    "\n",
    "            unseen_items = self.popularItems[unseen_items_mask]\n",
    "\n",
    "            recommended_items = unseen_items[0:at]\n",
    "\n",
    "        else:\n",
    "            recommended_items = self.popularItems[0:at]\n",
    "            \n",
    "        #recommended_items = \" \".join(str(i) for i in recommended_items)\n",
    "        return recommended_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBasedCollaborativeFilteringRecommender(object):\n",
    "    \n",
    "    def fit(self, URM_csr):\n",
    "        self.similarities = cosine_similarity(URM_csr, dense_output=False)\n",
    "        self.URM_csr = URM_csr\n",
    "    \n",
    "    def recommend(self, user_id, at=10, remove_seen=True):\n",
    "        sim = np.array(self.similarities.getrow(user_id).todense())[0]\n",
    "        itemPopularity = self.URM_csr.transpose().dot(sim)\n",
    "        self.popularItems = np.argsort(itemPopularity)\n",
    "        self.popularItems = np.flip(self.popularItems, axis = 0)\n",
    "        \n",
    "        if remove_seen:\n",
    "            unseen_items_mask = np.in1d(self.popularItems, self.URM_csr[user_id].indices,\n",
    "                                        assume_unique=True, invert = True)\n",
    "\n",
    "            unseen_items = self.popularItems[unseen_items_mask]\n",
    "\n",
    "            recommended_items = unseen_items[0:at]\n",
    "\n",
    "        else:\n",
    "            recommended_items = self.popularItems[0:at]\n",
    "            \n",
    "        #recommended_items = \" \".join(str(i) for i in recommended_items)\n",
    "        return recommended_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "    \n",
    "    return precision_score\n",
    "\n",
    "def recall(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall_score\n",
    "\n",
    "def MAP(recommended_items, relevant_items):\n",
    "       \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    \n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommender_object, target_playlists, at=10):\n",
    "    \n",
    "    \n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "\n",
    "\n",
    "    for user_id in target_playlists:\n",
    "    \n",
    "        target_items = URM_test.getrow(user_id).indices\n",
    "        \n",
    "        recommended_items = recommender_object.recommend(user_id, at=at)\n",
    "        num_eval+=1\n",
    "        \n",
    "        cumulative_precision += precision(recommended_items, target_items)\n",
    "        cumulative_recall += recall(recommended_items, target_items)\n",
    "        cumulative_MAP += MAP(recommended_items, target_items)\n",
    "\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "    \n",
    "    print(\"Recommender performance is: Precision = {:.6f}, Recall = {:.6f}, MAP = {:.6f}\".format(\n",
    "        cumulative_precision, cumulative_recall, cumulative_MAP)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender performance is: Precision = 0.013676, Recall = 0.013676, MAP = 0.004755\n"
     ]
    }
   ],
   "source": [
    "top_recommender = TopPopRecommender()\n",
    "top_recommender.fit(training_set_csr)\n",
    "\n",
    "evaluate_algorithm(test_set_csr, top_recommender, test_set_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recommender = UserBasedCollaborativeFilteringRecommender()\n",
    "user_recommender.fit(training_set_csr)\n",
    "\n",
    "evaluate_algorithm(test_set_csr, user_recommender, test_set_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
